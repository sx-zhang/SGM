import numpy as np
import semexp.envs.utils.depth_utils as du
import torch
import torch.nn as nn

from semexp.utils.distributions import Categorical, DiagGaussian
from semexp.utils.model import ChannelPool, Flatten, get_grid, NNBase
from torch.nn import functional as F


class Goal_Oriented_Semantic_Policy(NNBase):
    def __init__(
        self,
        input_shape,
        recurrent=False,
        hidden_size=512,
        num_sem_categories=16,
        main_model="simple_cnn",
    ):
        super(Goal_Oriented_Semantic_Policy, self).__init__(
            recurrent, hidden_size, hidden_size
        )

        self.main_model = main_model

        if main_model == "simple_cnn":
            out_size = int(input_shape[1] / 16.0) * int(input_shape[2] / 16.0) * 32
            extra_embed = 8 * 2
            self.main = nn.Sequential(
                nn.MaxPool2d(2),
                nn.Conv2d(num_sem_categories + 8, 32, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 64, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, 3, stride=1, padding=1),
                nn.ReLU(),
                Flatten(),
            )

        elif main_model == "ans":
            extra_embed = 8
            out_size = int(input_shape[1] / 16.0) * int(input_shape[2] / 16.0) * 32
            self.main = nn.Sequential(
                nn.MaxPool2d(2),
                nn.Conv2d(8, 32, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(32, 64, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(64, 128, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.MaxPool2d(2),
                nn.Conv2d(128, 64, 3, stride=1, padding=1),
                nn.ReLU(),
                nn.Conv2d(64, 32, 3, stride=1, padding=1),
                nn.ReLU(),
                Flatten(),
            )

        else:
            raise ValueError(f"=====> Model {main_model} not defined!")

        self.linear1 = nn.Linear(out_size + extra_embed, hidden_size)
        self.linear2 = nn.Linear(hidden_size, 256)
        self.critic_linear = nn.Linear(256, 1)
        self.orientation_emb = nn.Embedding(72, 8)
        if self.main_model != "ans":
            self.goal_emb = nn.Embedding(num_sem_categories, 8)
        self.train()

    def forward(self, inputs, rnn_hxs, masks, extras):
        if self.main_model == "ans":
            inputs = inputs[:, :8]
        x = self.main(inputs)
        orientation_emb = self.orientation_emb(extras[:, 0])
        if self.main_model == "ans":
            x = torch.cat((x, orientation_emb), 1)
        else:
            goal_emb = self.goal_emb(extras[:, 1])
            x = torch.cat((x, orientation_emb, goal_emb), 1)

        x = nn.ReLU()(self.linear1(x))
        if self.is_recurrent:
            x, rnn_hxs = self._forward_gru(x, rnn_hxs, masks)

        x = nn.ReLU()(self.linear2(x))

        return self.critic_linear(x).squeeze(-1), x, rnn_hxs


# https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/model.py#L15
class RL_Policy(nn.Module):
    def __init__(self, obs_shape, action_space, model_type=0, base_kwargs=None):

        super(RL_Policy, self).__init__()
        if base_kwargs is None:
            base_kwargs = {}

        if model_type == 1:
            self.network = Goal_Oriented_Semantic_Policy(obs_shape, **base_kwargs)
        else:
            raise NotImplementedError

        if action_space.__class__.__name__ == "Discrete":
            num_outputs = action_space.n
            self.dist = Categorical(self.network.output_size, num_outputs)
        elif action_space.__class__.__name__ == "Box":
            num_outputs = action_space.shape[0]
            self.dist = DiagGaussian(self.network.output_size, num_outputs)
        else:
            raise NotImplementedError

        self.model_type = model_type

    @property
    def is_recurrent(self):
        return self.network.is_recurrent

    @property
    def rec_state_size(self):
        """Size of rnn_hx."""
        return self.network.rec_state_size

    def forward(self, inputs, rnn_hxs, masks, extras):
        if extras is None:
            return self.network(inputs, rnn_hxs, masks)
        else:
            return self.network(inputs, rnn_hxs, masks, extras)

    def act(self, inputs, rnn_hxs, masks, extras=None, deterministic=False):

        value, actor_features, rnn_hxs = self(inputs, rnn_hxs, masks, extras)
        dist = self.dist(actor_features)

        if deterministic:
            action = dist.mode()
        else:
            action = dist.sample()

        action_log_probs = dist.log_probs(action)

        return value, action, action_log_probs, rnn_hxs

    def get_value(self, inputs, rnn_hxs, masks, extras=None):
        value, _, _ = self(inputs, rnn_hxs, masks, extras)
        return value

    def evaluate_actions(self, inputs, rnn_hxs, masks, action, extras=None):

        value, actor_features, rnn_hxs = self(inputs, rnn_hxs, masks, extras)
        dist = self.dist(actor_features)

        action_log_probs = dist.log_probs(action)
        dist_entropy = dist.entropy().mean()

        return value, action_log_probs, dist_entropy, rnn_hxs


class Semantic_Mapping(nn.Module):

    """
    Semantic_Mapping
    """

    def __init__(self, args):
        super(Semantic_Mapping, self).__init__()

        self.device = args.device
        self.screen_h = args.frame_height
        self.screen_w = args.frame_width
        self.resolution = args.map_resolution
        self.z_resolution = args.map_resolution
        self.map_size_cm = args.map_size_cm // args.global_downscaling
        self.n_channels = 3
        self.vision_range = args.vision_range
        self.dropout = 0.5
        self.fov = args.hfov
        self.du_scale = args.du_scale
        self.cat_pred_threshold = args.cat_pred_threshold
        self.exp_pred_threshold = args.exp_pred_threshold
        self.map_pred_threshold = args.map_pred_threshold
        self.num_sem_categories = args.num_sem_categories

        self.max_height = int(360 / self.z_resolution)
        self.min_height = int(-40 / self.z_resolution)
        self.agent_height = args.camera_height * 100.0
        self.shift_loc = [self.vision_range * self.resolution // 2, 0, np.pi / 2.0]
        self.camera_matrix = du.get_camera_matrix(
            self.screen_w, self.screen_h, self.fov
        )

        self.pool = ChannelPool(1)
        # -----------------------------------------------------------------------
        # Optimizing implementation
        # -----------------------------------------------------------------------
        self.agent_view = None
        self.grids = None

        vr = self.vision_range

        self.init_grid = (
            torch.zeros(
                args.num_processes,
                1 + self.num_sem_categories,
                vr,
                vr,
                self.max_height - self.min_height,
            )
            .float()
            .to(self.device)
        )
        self.feat = (
            torch.ones(
                args.num_processes,
                1 + self.num_sem_categories,
                self.screen_h // self.du_scale * self.screen_w // self.du_scale,
            )
            .float()
            .to(self.device)
        )

    # def forward(self, obs, pose_obs, maps_last, poses_last):
    #     bs, c, h, w = obs.size() # yxy 1 20 120 160
    #     depth = obs[:, 3, :, :]

    #     # -----------------------------------------------------------------------
    #     # Optimizing implementation
    #     # -----------------------------------------------------------------------
    #     if self.grids is None or self.grids[0].shape[0] != bs:
    #         self.grids = du.get_meshgrid(depth, self.device)

    #     point_cloud_t = du.get_point_cloud_from_z_t(
    #         depth,
    #         self.camera_matrix,
    #         self.device,
    #         scale=self.du_scale,
    #         grids=self.grids,
    #     )

    #     agent_view_t = du.transform_camera_view_t(
    #         point_cloud_t, self.agent_height, 0, self.device
    #     )

    #     agent_view_centered_t = du.transform_pose_t(
    #         agent_view_t, self.shift_loc, self.device
    #     )

    #     max_h = self.max_height
    #     min_h = self.min_height
    #     xy_resolution = self.resolution
    #     z_resolution = self.z_resolution
    #     vision_range = self.vision_range
    #     XYZ_cm_std = agent_view_centered_t.float()
    #     XYZ_cm_std[..., :2].div_(xy_resolution)
    #     XYZ_cm_std[..., :2].sub_(vision_range // 2.0).div_(vision_range / 2.0)
    #     XYZ_cm_std[..., 2].div_(z_resolution)
    #     XYZ_cm_std[..., 2].sub_((max_h + min_h) // 2.0).div_((max_h - min_h) / 2.0)
    #     self.feat[:, 1:, :] = nn.AvgPool2d(self.du_scale)(obs[:, 4:, :, :]).view(
    #         bs, c - 4, h // self.du_scale * w // self.du_scale
    #     ) # 1 17 19200

    #     XYZ_cm_std = XYZ_cm_std.permute(0, 3, 1, 2)
    #     XYZ_cm_std = XYZ_cm_std.view(
    #         XYZ_cm_std.shape[0],
    #         XYZ_cm_std.shape[1],
    #         XYZ_cm_std.shape[2] * XYZ_cm_std.shape[3],
    #     ) # 1 3 19200 yxy

    #     voxels = du.splat_feat_nd(
    #         self.init_grid * 0.0, self.feat, XYZ_cm_std
    #     ).transpose(2, 3) # 1 17 100 100 80

    #     min_z = int(25 / z_resolution - min_h)
    #     max_z = int((self.agent_height + 1) / z_resolution - min_h)

    #     agent_height_proj = voxels[..., min_z:max_z].sum(4)
    #     all_height_proj = voxels.sum(4)

    #     fp_map_pred = agent_height_proj[:, 0:1, :, :]
    #     fp_exp_pred = all_height_proj[:, 0:1, :, :]
    #     fp_map_pred = fp_map_pred / self.map_pred_threshold
    #     fp_exp_pred = fp_exp_pred / self.exp_pred_threshold
    #     fp_map_pred = torch.clamp(fp_map_pred, min=0.0, max=1.0)
    #     fp_exp_pred = torch.clamp(fp_exp_pred, min=0.0, max=1.0)

    #     pose_pred = poses_last

    #     if self.agent_view is None or self.agent_view.shape[0] != bs:
    #         self.agent_view = torch.zeros(
    #             bs,
    #             c,
    #             self.map_size_cm // self.resolution,
    #             self.map_size_cm // self.resolution,
    #         ).to(self.device)
    #     else:
    #         self.agent_view.fill_(0)
    #     agent_view = self.agent_view

    #     x1 = self.map_size_cm // (self.resolution * 2) - self.vision_range // 2
    #     x2 = x1 + self.vision_range
    #     y1 = self.map_size_cm // (self.resolution * 2)
    #     y2 = y1 + self.vision_range
    #     agent_view[:, 0:1, y1:y2, x1:x2] = fp_map_pred
    #     agent_view[:, 1:2, y1:y2, x1:x2] = fp_exp_pred
    #     agent_view[:, 4:, y1:y2, x1:x2] = torch.clamp(
    #         agent_height_proj[:, 1:, :, :] / self.cat_pred_threshold, min=0.0, max=1.0
    #     )

    #     corrected_pose = pose_obs

    #     current_poses = self.get_new_pose_batch(poses_last, corrected_pose)
    #     st_pose = current_poses.clone().detach()

    #     st_pose[:, :2] = -(
    #         st_pose[:, :2] * 100.0 / self.resolution
    #         - self.map_size_cm // (self.resolution * 2)
    #     ) / (self.map_size_cm // (self.resolution * 2))
    #     st_pose[:, 2] = 90.0 - (st_pose[:, 2])

    #     rot_mat, trans_mat = get_grid(st_pose, agent_view.size(), self.device)

    #     rotated = F.grid_sample(agent_view, rot_mat, align_corners=True)
    #     translated = F.grid_sample(rotated, trans_mat, align_corners=True)

    #     maps2 = torch.cat((maps_last.unsqueeze(1), translated.unsqueeze(1)), 1)
        
    #     map_pred, _ = torch.max(maps2, 1)

    #     return fp_map_pred, map_pred, pose_pred, current_poses, translated
    
    def forward(self, obs, pose_obs, maps_last, poses_last):
        bs, c, h, w = obs.size()
        depth = obs[:, 3, :, :]

        # -----------------------------------------------------------------------
        # Optimizing implementation
        # -----------------------------------------------------------------------
        if self.grids is None or self.grids[0].shape[0] != bs:
            self.grids = du.get_meshgrid(depth, self.device)

        point_cloud_t = du.get_point_cloud_from_z_t(
            depth,
            self.camera_matrix,
            self.device,
            scale=self.du_scale,
            grids=self.grids,
        )

        agent_view_t = du.transform_camera_view_t(
            point_cloud_t, self.agent_height, 0, self.device
        )

        agent_view_centered_t = du.transform_pose_t(
            agent_view_t, self.shift_loc, self.device
        )

        max_h = self.max_height
        min_h = self.min_height
        xy_resolution = self.resolution
        z_resolution = self.z_resolution
        vision_range = self.vision_range
        XYZ_cm_std = agent_view_centered_t.float()
        XYZ_cm_std[..., :2].div_(xy_resolution)
        XYZ_cm_std[..., :2].sub_(vision_range // 2.0).div_(vision_range / 2.0)
        XYZ_cm_std[..., 2].div_(z_resolution)
        XYZ_cm_std[..., 2].sub_((max_h + min_h) // 2.0).div_((max_h - min_h) / 2.0)
        self.feat[:, 1:, :] = nn.AvgPool2d(self.du_scale)(obs[:, 4:, :, :]).view(
            bs, c - 4, h // self.du_scale * w // self.du_scale
        )

        XYZ_cm_std = XYZ_cm_std.permute(0, 3, 1, 2)
        XYZ_cm_std = XYZ_cm_std.view(
            XYZ_cm_std.shape[0],
            XYZ_cm_std.shape[1],
            XYZ_cm_std.shape[2] * XYZ_cm_std.shape[3],
        )

        voxels = du.splat_feat_nd(
            self.init_grid * 0.0, self.feat, XYZ_cm_std
        ).transpose(2, 3)

        min_z = int(25 / z_resolution - min_h)
        max_z = int((self.agent_height + 1) / z_resolution - min_h)

        agent_height_proj = voxels[..., min_z:max_z].sum(4)
        all_height_proj = voxels.sum(4)

        fp_map_pred = agent_height_proj[:, 0:1, :, :]
        fp_exp_pred = all_height_proj[:, 0:1, :, :]
        fp_map_pred = fp_map_pred / self.map_pred_threshold
        fp_exp_pred = fp_exp_pred / self.exp_pred_threshold
        fp_map_pred = torch.clamp(fp_map_pred, min=0.0, max=1.0)
        fp_exp_pred = torch.clamp(fp_exp_pred, min=0.0, max=1.0)

        pose_pred = poses_last

        if self.agent_view is None or self.agent_view.shape[0] != bs:
            self.agent_view = torch.zeros(
                bs,
                c,
                self.map_size_cm // self.resolution,
                self.map_size_cm // self.resolution,
            ).to(self.device)
        else:
            self.agent_view.fill_(0)
        agent_view = self.agent_view

        x1 = self.map_size_cm // (self.resolution * 2) - self.vision_range // 2
        x2 = x1 + self.vision_range
        y1 = self.map_size_cm // (self.resolution * 2)
        y2 = y1 + self.vision_range
        agent_view[:, 0:1, y1:y2, x1:x2] = fp_map_pred
        agent_view[:, 1:2, y1:y2, x1:x2] = fp_exp_pred
        agent_view[:, 4:, y1:y2, x1:x2] = torch.clamp(
            agent_height_proj[:, 1:, :, :] / self.cat_pred_threshold, min=0.0, max=1.0
        )

        corrected_pose = pose_obs

        current_poses = self.get_new_pose_batch(poses_last, corrected_pose)
        st_pose = current_poses.clone().detach()

        st_pose[:, :2] = -(
            st_pose[:, :2] * 100.0 / self.resolution
            - self.map_size_cm // (self.resolution * 2)
        ) / (self.map_size_cm // (self.resolution * 2))
        st_pose[:, 2] = 90.0 - (st_pose[:, 2])

        rot_mat, trans_mat = get_grid(st_pose, agent_view.size(), self.device)

        rotated = F.grid_sample(agent_view, rot_mat, align_corners=True)
        translated = F.grid_sample(rotated, trans_mat, align_corners=True)

        maps2 = torch.cat((maps_last.unsqueeze(1), translated.unsqueeze(1)), 1)

        map_pred, _ = torch.max(maps2, 1)

        return fp_map_pred, map_pred, pose_pred, current_poses

    def get_new_pose_batch(self, pose, rel_pose_change):

        pose[:, 1] += rel_pose_change[:, 0] * torch.sin(
            pose[:, 2] / 57.29577951308232
        ) + rel_pose_change[:, 1] * torch.cos(pose[:, 2] / 57.29577951308232)
        pose[:, 0] += rel_pose_change[:, 0] * torch.cos(
            pose[:, 2] / 57.29577951308232
        ) - rel_pose_change[:, 1] * torch.sin(pose[:, 2] / 57.29577951308232)
        pose[:, 2] += rel_pose_change[:, 2] * 57.29577951308232

        pose[:, 2] = torch.fmod(pose[:, 2] - 180.0, 360.0) + 180.0
        pose[:, 2] = torch.fmod(pose[:, 2] + 180.0, 360.0) - 180.0

        return pose
